# Master katuali configuration file template
# ------------------------------------------
#
# Having copied this file with the command:
#     katuali_config my_config.yaml
# it will be necessary to amend the `DATA:` dictionary in the first section below
# to define the dataset to be processed.
#
# The second section containing software locations should also be checked and
# amended if necessary.
#
# To run the predefined workflows: fast_assm_polish standard_assm_polish
# standard_assm_nanopolish all_medaka_train_features starting from fast5 input,
# nothing else need be done. If starting a workflow from basecall data
# (https://nanoporetech.github.io/katuali/examples.html#starting-from-existing-basecalls),
# basecalls should be placed under a folder as determined by the value of the
# `BASECALLER` in the pipeline options.


####################
# Input data options

# .fast5 files can be under top-level folders named by a RUNID (these need not
# be actual run UUIDs). Fast5s should be in RUNID/reads.  The keys within this
# data dictionary are RUNIDs.
#
# REFERENCE is required for the medaka training workflows, and in general any
# time reads need to be aligned to a reference (e.g. for for subsampling)
# and for calling variants. 
#   
# MEDAKA_TRAIN_REGIONS and MEDAKA_EVAL_REGIONS define regions for
# training and evaluation within the medaka training workflow, and are
# otherwise not used. 
#
# MEDAKA_VARIANT_REGIONS is used in the medaka_all_variant workflow. 
#
# GENOME_SIZE is required only in workflows using the canu or flye assembler.
#
# In the example below we train from the "minion" run using "ecoli" and "yeast"
# contigs in the reference and evaluate on the "gridion" run using the contigs
# "ecoli", "yeast" and "na12878_chr21" in the reference.
DATA:
    'MinIonRun1': 
        'GENOME_SIZE': '4.8M'
        'MEDAKA_TRAIN_REGIONS': ['ecoli', 'yeast']
        'MEDAKA_EVAL_REGIONS': []
        'MEDAKA_VARIANT_REGIONS': []
        'REFERENCE': 'ref.fasta'
    'MinIonRun2': 
        'GENOME_SIZE': '4.8M'
        'MEDAKA_TRAIN_REGIONS': ['ecoli', 'yeast']
        'MEDAKA_EVAL_REGIONS': []
        'MEDAKA_VARIANT_REGIONS': []
        'REFERENCE': 'ref.fasta'   
    'GridIonRun1': 
        'GENOME_SIZE': '4.8M'
        'MEDAKA_TRAIN_REGIONS': []
        'MEDAKA_EVAL_REGIONS': ['ecoli', 'yeast', 'na12878_chr21']
        'MEDAKA_VARIANT_REGIONS': []
        'REFERENCE': 'ref.fasta'   
    'GridIonRun2': 
        'GENOME_SIZE': '4.8M'
        'MEDAKA_TRAIN_REGIONS': []
        'MEDAKA_EVAL_REGIONS': ['ecoli', 'yeast', 'na12878_chr21']
        'MEDAKA_VARIANT_REGIONS': []
        'REFERENCE': 'ref.fasta'   


##############################
# Location of various software
#
# Configuration for software is found toward the end of this config.

# Katuali supports running medaka consensus only on CPU and training only on
# GPU. If you wish to train and run consensus it is easiest to create two
# medaka virtual environemnts - one for GPU (with tensorflow-gpu installed) and
# one for CPU (with just tensorflow installed). 
IN_MEDAKA: "~/git/medaka/venv/bin/activate"  # medaka used for consensus - should only have tensorflow installed
IN_MEDAKA_GPU: "~/git/medaka_gpu/venv/bin/activate"  # medaka used for training - should have tensorflow-gpu installed
IN_POMOXIS: "~/git/pomoxis/venv/bin/activate"
IN_RAY: "~/git/ray/venv/bin/activate"
CANU_EXEC: "~/git/canu-1.8/Linux-amd64/bin/canu"
FLYE_EXEC: "~/git/Flye/bin/flye"
NANOPOLISH: "~/git/nanopolish" 
SCRAPPIE: "~/git/scrappie"
FLAPPIE: "~/git/github/flappie"
GUPPY: "/usr/bin/guppy_basecaller"
SOURCE: "source"
LD_LIBRARY_PATH: ""  # set this for e.g. CUDA library path


##################
# Pipeline options

# Basecaller for predefined workflows. The `fast_assm_polish`,
# `standard_assm_polish` `standard_assm_nanopolish` and medaka training
# workflows use this value to determine the basecaller setup to use.
#
# It is recommended to set this to: guppy to use the guppy basecaller
BASECALLER: "guppy"
# BASECALLER_SUFFIX controls which guppy suffix and medaka suffix gets used in pipeline targets
# it is recommended to set this to either _hac (high accuracy minion) or _hac_prom (high accuracy promethion)
BASECALLER_SUFFIX: "_hac" 

# Assembler for predefined workflows ("flye" or canu""). The `fast_assm_polish`,
# `standard_assm_polish` `standard_assm_nanopolish` and medaka training
# workflows use this value to determine the asembler setup to use.
ASSEMBLER: "canu"


# Pipeline target template definitions containing other config parameters that
# will be filled in with config variables to generate targets, see
# documentation for full details.  

PIPELINES:
    fast_assm_polish: [
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/miniasm_racon/medaka{BASECALLER_SUFFIX}/consensus.fasta"
    ]
    standard_assm_polish: [
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/{ASSEMBLER}_gsz_{GENOME_SIZE}/racon/medaka{BASECALLER_SUFFIX}/consensus.fasta"
    ]
    standard_assm_nanopolish: [
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/{ASSEMBLER}_gsz_{GENOME_SIZE}/racon/nanopolish_hp/consensus.fasta"
    ]
    medaka_eval: [
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/align/{MEDAKA_EVAL_REGIONS}/{DEPTHS}X/{ASSEMBLER}_gsz_{GENOME_SIZE}/racon/medaka{MEDAKA_EVAL_SUFFIXES}/consensus_to_truth_summ.txt",
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/align/{MEDAKA_EVAL_REGIONS}/{DEPTHS}X/{ASSEMBLER}_gsz_{GENOME_SIZE}/racon/consensus_to_truth_summ.txt"
    ]
    medaka_feat: [
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/align/{MEDAKA_TRAIN_REGIONS}/{DEPTHS}X_prop/{ASSEMBLER}_gsz_{GENOME_SIZE}/racon/medaka_train/medaka_train.hdf",
        "{DATA}/basecall/{BASECALLER}{BASECALLER_SUFFIX}/align/{MEDAKA_TRAIN_REGIONS}/{DEPTHS}X_prop/canu_gsz_{GENOME_SIZE}/racon/medaka_train/medaka_train_rc.hdf"
    ]
    medaka_train: [
        "medaka_train_{MEDAKA_TRAIN_REPLICATES}"
    ]



#########################
# Compute/cluster options

# Location of scratch space on cluster nodes. This is currently only used for
# medaka training where latency on an networked filesystem can significantly
# slow things down. To have katuali copy medaka features to scratch space
# before training set SCRATCH and optionally TMPSCRATCH.
#
# If TMPSCRATCH is given the resultant path will be:
#     /SCRATCH/$USER/TMPSCRATCH
# whereas if it is not given the resultant path will be:
#    /SCRATCH/$USER/<generated unique name>
#
# TMPSCRATCH is useful in the case of restarting a training job on a node to
# which features have already been copied; in this case, simply set TMPSCRATCH
# to the directory under which the features were copied and katuali will skip
# copying the data and use the existing data.
#
# Note that katuali does not delete the data in scratch, so users must do this
# manually.
SCRATCH: ""
TMPSCRATCH: ""

# default number of threads/cores/slots to use per multi-threaded task.
THREADS_PER_JOB: 16


#############################
# Options for medaka training

# This section can be ignored if not running a medaka training workflow

# Read depths at which to create assemblies for training, this should
# span the range of depths at which the model is to be used
# Use the --keep-going option of Snakemake if you are happy the relax
# the constraint of requiring all depths for all reference sequences. 
DEPTHS:
    [25, 50, 75, 100, 125, 150, 175, 200]

# if any of your medaka feature files are misssing (e.g. due to insufficient coverage for some runs / contigs)
# the medaka training step will not find all the features it expects and will not run. 
# If you want to just train on the feauture files which did get created, you can set this flag to true
# (after having already created the features with the flag set to false) 
USE_ONLY_EXISTING_MEDAKA_FEAT: false

# Run multiple training replicates - output will be in medaka_train_{key}, values should be a key of PIPELINES
# This allows training of multiple models each using different training features 
MEDAKA_TRAIN_REPLICATES:
    "rep_1": "medaka_feat" 
    "rep_2": "medaka_feat"
    "rep_3": "medaka_feat"


# to test multiple trained models, use multiple suffixes. 
# The suffix should be a key within MEDAKA_OPTS specifying the path of a trained model 
MEDAKA_EVAL_SUFFIXES: ["_rep_1_best_val", "_rep_2_best_val", "_rep_3_best_val"]
MEDAKA_OPTS: 
    "": "-m r941_min_high"
    "_hac": "-m r941_min_high"
    "_hac_prom": "-m r941_prom_high"
    "_fast": "-m r941_min_fast"
    "_fast_prom": "-m r941_prom_fast"
    "_rep_1_best_val": "-m medaka_train_rep1/model.best.val.hdf5"
    "_rep_2_best_val": "-m medaka_train_rep2/model.best.val.hdf5"
    "_rep_3_best_val": "-m medaka_train_rep3/model.best.val.hdf5"

# Medaka training features
MEDAKA_TRAIN_TRUTH_CHUNK: 10000  # this sets the size of truth chunks aligned to draft to provide labels. 
MEDAKA_TRAIN_FEAT_OPTS:
    "": "--chunk_len 1000 --chunk_ovlp 0"

MEDAKA_TRAIN_OPTS:
    "": "--mini_epochs 5 --validation_split 0.10"

# Medaka training can be IO-limited - using mult-threading to load data can speed things up
MEDAKA_TRAIN_THREADS_IO: 8

MEDAKA_VARIANT_OPTS:
    "_hac": "-m r941_min_high -d"

MEDAKA_VARIANT_SUFFIXES: ["_hac"]


############################
# Misc. options for programs
#
# For advanced users. See
# https://nanoporetech.github.io/katuali/examples.html#examples
# for help in contructing bespoke pipelines.
#
# For fully specified targets, rather than predefined workflows the below can
# be used to manipulate how each program is run. Each key acts as a shortcut
# for a specific parameter set (as well as functioning as a target name).
# Default parameters are given by the `""` targets

# Scrappie 
SCRAPPIE_OPTS: 
    "": "raw --model rgrgr_r94 --uuid"
    "_hp": "raw -H mean --model rgrgr_r94 --uuid"
        
# Flappie 
FLAPPIE_OPTS: 
    "": "raw --model rgrgr_r94 --uuid"
    "_r10c_pcr": "--model=r10c_pcr"

# Guppy        
GUPPY_OPTS: 
    "": "-c dna_r9.4.1_450bps_hac.cfg"
    "_hac": "-c dna_r9.4.1_450bps_hac.cfg"
    "_hac_prom": "-c dna_r9.4.1_450bps_hac_prom.cfg"
    "_fast": "-c dna_r9.4.1_450bps_fast.cfg"
    "_fast_prom": "-c dna_r9.4.1_450bps_fast_prom.cfg"

GUPPY_SLOTS: 8  # for passing to SGE parallel environment

# subsample_bam
SUBSAMPLE_BAM_OPTS:
    "": " --all_fail"
    "_prop": "--proportional --all_fail"
    "_filtered": "--quality 6 --coverage 90 --all_fail"

# mini_assemble
MINI_ASSEMBLE_OPTS: 
    "": ""
    "_ce": "-c -e 10"
    "_ces": "-c -e 10 -n 10"
 
# Canu 
CANU_EXEC_OPTS: "useGrid=False" # this shouldn't be changed, snakemake will be confused
CANU_OPTS:
    "": "" 

# Flye 
FLYE_OPTS:
    "": "" 

# Nanopolish
# wildcards in dynamic files cannot be constrained => we can't safely extract a
# suffix from dynamic nanopolish targets since they use nested config
NANOPOLISH_OPTS: "--fix-homopolymers"

# assess_assembly
ASSESS_ASSM_OPTS: "-C"

